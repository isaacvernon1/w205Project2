{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Final Table\n",
    "\n",
    "In this part of the code, I will be going through the pyspark code used to create a table that can be SQL called/put into an HDFS. Comments will be in the form of comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do necessary imports\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Read in Data From Kafka\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cached to cut back on later warnings\n",
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cast it as a string\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize what we have currently\n",
    "assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Questions:\n",
    "\n",
    "1) How many people took each exam? -- Create unrolled table, can answer overall size, etc\n",
    "\n",
    "2) What is the average and standard deviation of scores on all exams? -- Create Scores/Questions Table\n",
    "\n",
    "3) For a particular user_exam_id (use '6d4089e4-bde5-4a22-b65f-18bce9ab79c8' as an example, what are the id's of the questions asked? Sequence Id? -- Create Questions table with sequence id, user_exam_id, question id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3280|\n",
      "+--------+\n",
      "\n",
      "+--------+--------------------+\n",
      "|count(1)|           exam_name|\n",
      "+--------+--------------------+\n",
      "|       9|Learning Data Mod...|\n",
      "|      15|Networking for Pe...|\n",
      "|     158|Introduction to J...|\n",
      "|      16|Learning Apache H...|\n",
      "|       2|Learning Spring P...|\n",
      "|      17|Learning iPython ...|\n",
      "|     162|Introduction to P...|\n",
      "|      35|Learning C# Best ...|\n",
      "|      14|Introduction to A...|\n",
      "|       9|A Practical Intro...|\n",
      "|      15|I'm a Software Ar...|\n",
      "|      75|Introduction to B...|\n",
      "|       4|       View Updating|\n",
      "|      25|Mastering Python ...|\n",
      "|      43|Intermediate C# P...|\n",
      "|       5|Starting a Grails...|\n",
      "|       9|Introduction to A...|\n",
      "|      21|JavaScript Templa...|\n",
      "|      10|Being a Better In...|\n",
      "|      34|Mastering Advance...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+------------+\n",
      "|count(1)|   exam_name|\n",
      "+--------+------------+\n",
      "|     394|Learning Git|\n",
      "+--------+------------+\n",
      "\n",
      "+--------+--------------------+\n",
      "|count(1)|           exam_name|\n",
      "+--------+--------------------+\n",
      "|       1|Learning to Visua...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1 Work\n",
    "\n",
    "# Unroll assessments from the top (will have nesting in sequences column, don't use for anything there)\n",
    "# Best for questions only involving outermost layer of Json: base_exam_id to user_exam_id level \n",
    "# Called onelayerassessments due to this structure\n",
    "unroll_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "unroll_assessments.registerTempTable('assessments')\n",
    "\n",
    "# Get how many assessments, count by assessment\n",
    "spark.sql(\"select count(*) from assessments\").show()\n",
    "spark.sql(\"select count(*), exam_name from assessments group by exam_name\").show()\n",
    "\n",
    "# To summarize how many people took each exam, find max, min\n",
    "spark.sql(\"select count(*), exam_name from assessments group by exam_name order by count(*) desc limit 1\").show()\n",
    "spark.sql(\"select count(*), exam_name from assessments group by exam_name order by count(*) asc limit 1\").show()\n",
    "\n",
    "# Write to HDFS\n",
    "unroll_assessments.write.parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+-----+\n",
      "|correct|           exam_name|percent|total|\n",
      "+-------+--------------------+-------+-----+\n",
      "|      2|Normal Forms and ...|   50.0|    4|\n",
      "|      1|Normal Forms and ...|   25.0|    4|\n",
      "|      3|The Principles of...|   75.0|    4|\n",
      "|      2|The Principles of...|   50.0|    4|\n",
      "|      3|Introduction to B...|   75.0|    4|\n",
      "|      5|        Learning Git|  100.0|    5|\n",
      "|      1|Git Fundamentals ...|  100.0|    1|\n",
      "|      5|Introduction to P...|  100.0|    5|\n",
      "|      4|Intermediate Pyth...|  100.0|    4|\n",
      "|      0|Introduction to P...|    0.0|    5|\n",
      "+-------+--------------------+-------+-----+\n",
      "\n",
      "+------------------+\n",
      "|      avg(percent)|\n",
      "+------------------+\n",
      "|62.656997455470844|\n",
      "+------------------+\n",
      "\n",
      "+--------------------+\n",
      "|stddev_samp(percent)|\n",
      "+--------------------+\n",
      "|  31.086692286170475|\n",
      "+--------------------+\n",
      "\n",
      "+--------+------------------+\n",
      "|count(1)|           percent|\n",
      "+--------+------------------+\n",
      "|     239|               0.0|\n",
      "|       3|              10.0|\n",
      "|       7|              12.5|\n",
      "|      21|14.285714285714286|\n",
      "|       8|16.666666666666668|\n",
      "|      85|              20.0|\n",
      "|     272|              25.0|\n",
      "|      21|28.571428571428573|\n",
      "|      84|33.333333333333336|\n",
      "|       3|              37.5|\n",
      "|     123|              40.0|\n",
      "|      23|42.857142857142854|\n",
      "|     442|              50.0|\n",
      "|       1|              55.0|\n",
      "|      14|57.142857142857146|\n",
      "|     174|              60.0|\n",
      "|       9|              62.5|\n",
      "|     103| 66.66666666666667|\n",
      "|       3|              70.0|\n",
      "|      25| 71.42857142857143|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+--------------------+\n",
      "|      avg(percent)|           exam_name|\n",
      "+------------------+--------------------+\n",
      "|58.333333333333336|A Practical Intro...|\n",
      "| 72.38805970149254|Advanced Machine ...|\n",
      "|              50.0|Amazon Web Servic...|\n",
      "| 72.72727272727273|Amazon Web Servic...|\n",
      "|              60.0|An Introduction t...|\n",
      "| 59.09090909090909|An Introduction t...|\n",
      "|              58.0|Architectural Con...|\n",
      "| 64.58333333333333|      Arduino Inputs|\n",
      "|              68.0|Arduino Prototypi...|\n",
      "|33.333333333333336|Arduino Prototypi...|\n",
      "|55.526315789473685|Beginning C# Prog...|\n",
      "| 76.58227848101266|Beginning Program...|\n",
      "|              50.0|Being a Better In...|\n",
      "|41.666666666666664|Building Web Serv...|\n",
      "|              20.0|Client-Side Data ...|\n",
      "| 42.64705882352941|Cloud Computing W...|\n",
      "|              80.0|Cloud Native Arch...|\n",
      "| 73.33333333333333|Collaborating wit...|\n",
      "|60.714285714285715|Data Science with...|\n",
      "|49.193548387096776|Data Visualizatio...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2 Work\n",
    "# We remove those without values for correct, total from our table\n",
    "def lambda_scores(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                my_dict = {\"exam_name\": raw_dict[\"exam_name\"],\n",
    "                           \"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "                           \"percent\": 100 * raw_dict[\"sequences\"][\"counts\"][\"correct\"] / raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "my_scores = assessments.rdd.flatMap(lambda_scores).toDF()\n",
    "\n",
    "my_scores.registerTempTable('scores')\n",
    "\n",
    "# Show table produced\n",
    "spark.sql(\"select * from scores limit 10\").show()\n",
    "\n",
    "# Get sd, avg of scores to answer Q2\n",
    "spark.sql(\"select avg(percent) from scores\").show()\n",
    "spark.sql(\"select stddev(percent) from scores\").show()\n",
    "\n",
    "# Show the number of each score for perspective\n",
    "spark.sql(\"select count(*), percent from scores group by percent order by percent asc\").show()\n",
    "\n",
    "# Show percents for each exam\n",
    "spark.sql(\"select avg(percent), exam_name from scores group by exam_name order by exam_name asc\").show()\n",
    "\n",
    "# Write to HDFS\n",
    "my_scores.write.parquet(\"/tmp/scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|           exam_name|         question_id|question_number|         sequence_id|        user_exam_id|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|Normal Forms and ...|7a2ed6d3-f492-49b...|              1|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "|Normal Forms and ...|bbed4358-999d-446...|              2|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "|Normal Forms and ...|e6ad8644-96b1-461...|              3|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "|Normal Forms and ...|95194331-ac43-454...|              4|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "|Normal Forms and ...|95194331-ac43-454...|              1|5b28a462-7a3b-42e...|2fec1534-b41f-441...|\n",
      "|Normal Forms and ...|bbed4358-999d-446...|              2|5b28a462-7a3b-42e...|2fec1534-b41f-441...|\n",
      "|Normal Forms and ...|e6ad8644-96b1-461...|              3|5b28a462-7a3b-42e...|2fec1534-b41f-441...|\n",
      "|Normal Forms and ...|7a2ed6d3-f492-49b...|              4|5b28a462-7a3b-42e...|2fec1534-b41f-441...|\n",
      "|The Principles of...|b9ff2e88-cf9d-4bd...|              1|b370a3aa-bf9e-4c1...|8edbc8a8-4d26-429...|\n",
      "|The Principles of...|bec23e7b-4870-49f...|              2|b370a3aa-bf9e-4c1...|8edbc8a8-4d26-429...|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|           exam_name|         question_id|question_number|         sequence_id|        user_exam_id|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|Normal Forms and ...|7a2ed6d3-f492-49b...|              1|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "|Normal Forms and ...|bbed4358-999d-446...|              2|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "|Normal Forms and ...|e6ad8644-96b1-461...|              3|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "|Normal Forms and ...|95194331-ac43-454...|              4|5b28a462-7a3b-42e...|6d4089e4-bde5-4a2...|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 3 Work\n",
    "def lambda_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    my_count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        my_count += 1\n",
    "        my_dict = {\"user_exam_id\" : raw_dict[\"user_exam_id\"],\n",
    "                   \"exam_name\": raw_dict[\"exam_name\"],\n",
    "                   \"question_number\" : my_count, \n",
    "                   \"question_id\" : l[\"id\"],\n",
    "                   \"sequence_id\": raw_dict[\"sequences\"][\"id\"]\n",
    "                  }\n",
    "        my_list.append(Row(**my_dict))\n",
    "        \n",
    "    return my_list\n",
    "\n",
    "my_questions = assessments.rdd.flatMap(lambda_questions).toDF()\n",
    "\n",
    "my_questions.registerTempTable('questions')\n",
    "\n",
    "# Look at table created\n",
    "spark.sql(\"select * from questions limit 10\").show()\n",
    "\n",
    "# Answer Q3\n",
    "spark.sql(\"select * from questions where user_exam_id == '6d4089e4-bde5-4a22-b65f-18bce9ab79c8'\").show()\n",
    "\n",
    "# Write to HDFS\n",
    "my_questions.write.parquet(\"/tmp/questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create function to do all transforms, get to final dataframe, for after easy parts to finish\n",
    "\n",
    "\n",
    "# This was additional for fun/curiosity, not part of the actual project\n",
    "def lambda_transform(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "              \"base_exam_id\": raw_dict[\"base_exam_id\"]}\n",
    "    # Try to create final table without merges, else just keep and merge on keen_id\n",
    "    base_exam_id = raw_dict[\"base_exam_id\"]\n",
    "    #my_dict = {\"base_exam_id\": base_exam_id}\n",
    "    my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_table = assessments.rdd.flatMap(lambda_transform).toDF()\n",
    "\n",
    "my_final_table.registerTempTable('finaltable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+\n",
      "|        base_exam_id|correct|total|\n",
      "+--------------------+-------+-----+\n",
      "|37f0a30a-7464-11e...|      2|    4|\n",
      "|37f0a30a-7464-11e...|      1|    4|\n",
      "|4beeac16-bb83-4d5...|      3|    4|\n",
      "|4beeac16-bb83-4d5...|      2|    4|\n",
      "|6442707e-7488-11e...|      3|    4|\n",
      "|8b4488de-43a5-4ff...|      5|    5|\n",
      "|e1f07fac-5566-4fd...|      1|    1|\n",
      "|7e2e0b53-a7ba-458...|      5|    5|\n",
      "|1a233da8-e6e5-48a...|      4|    4|\n",
      "|7e2e0b53-a7ba-458...|      0|    5|\n",
      "+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from finaltable limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           exam_name|\n",
      "+--------------------+\n",
      "|Normal Forms and ...|\n",
      "|Normal Forms and ...|\n",
      "|The Principles of...|\n",
      "|The Principles of...|\n",
      "|Introduction to B...|\n",
      "|        Learning Git|\n",
      "|Git Fundamentals ...|\n",
      "|Introduction to P...|\n",
      "|Intermediate Pyth...|\n",
      "|Introduction to P...|\n",
      "|A Practical Intro...|\n",
      "|Git Fundamentals ...|\n",
      "|Introduction to M...|\n",
      "|   Python Epiphanies|\n",
      "|Introduction to P...|\n",
      "|Python Data Struc...|\n",
      "|Python Data Struc...|\n",
      "|Working with Algo...|\n",
      "|Learning iPython ...|\n",
      "|   Python Epiphanies|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "extracted_assessments.registerTempTable('assessments')\n",
    "\n",
    "spark.sql(\"select exam_name from assessments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
