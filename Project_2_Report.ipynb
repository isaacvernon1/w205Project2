{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is an overview of the work done in project 2. It starts with a highlight of some sections of the command line history (to show what the commands are doing) then continues onto an explanation of the pyspark code and finally concludes with three simple business questions being answered with queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publishing Data with Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first steps in this project are to publish the data into kafka so it can be consumed by others. Therefore, the first step is to set up the kafka queue and publish messages to it. However, before we do that, we need to start up our cluster so we can use kafka. This is done through the command line by the following code (navigating to the correct directory and starting up our cluster)\n",
    "```\n",
    "cd w205/project-2-isaacvernon1/  \n",
    "docker-compose up -d\n",
    "```\n",
    "This will start up the cluster specified in our docker-compose.yml file, which includes zookeeper (which kafka depends on), kafka, cloudera (for Hadoop), spark, and a mids image. Now that we have intialized our cluster, we can check to make sure everything is running properly with\n",
    "```\n",
    "docker-compose ps \n",
    "docker ps -a\n",
    "```\n",
    "We could also use the following code in another linux command line window to watch the logs from Kafka in real time as they come in (we need to make sure we are in the w205/project-2-isaacvernon1/ directory)\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```\n",
    "Now that we have our Kafka cluster started, we need to create a topic. I create a topic assessments, with the name coming from the fact that the data we are going to publish is assessment data (We can use the second line of code to determine if our topic was created successfully). \n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "```\n",
    "Now that we have created a kafka topic, it is time to publish some messages to the queue. We do so using our assessments data and kafkacat to write to the queue (second line of code will print the messages that were published to kafka to the command line).\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-isaacvernon1/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "\n",
    "docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
    "```\n",
    "Finally, we now have to set up our system so we can write our pyspark code in a Jupyter Notebook (much more convenient as compared to writing in console). We run the following code and eventually get a link where we replace the 0.0.0.0 with our external IP address from our Virtual Machine.\n",
    "```\n",
    "docker-compose exec spark bash\n",
    "# These two lines are in the spark shell, exit will quit for us\n",
    "ln -s /w205 w205\n",
    "exit\n",
    "\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```\n",
    "We are now set to run the rest of our pyspark commands in a Jupyter Notebook, which is SparkCode.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming Data with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our pyspark session set up in a Jupyter Notebook, we can consume the messages from Kafka. This is done through the following code. We can also cache this data to help prevent future warning messages as well as casting the messages as strings into a new dataframe to prepare for future code.\n",
    "```\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()\n",
    "\n",
    "raw_assessments.cache()\n",
    "\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Data and Creating Tables with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data into pyspark, we can now transform our intial data to create queryable dataframes. In this part, we will create 3 different data tables; assessments, scores, and questions. For the first data table, we want to create a table that contains some of the external Json layer information. This only requires the simplest transformation. With the following line of code, we can unroll the data to expose the outermost values (will still have nesting in sequences, so we can't use these values in the table). We can then register and name our data table as a temporary table \"assessments\" which we can later run SQL queries against.\n",
    "```\n",
    "unroll_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "unroll_assessments.registerTempTable('assessments')\n",
    "```\n",
    "Next, we will create the scores table. When we look at the structure of the nested Json and what fields we may be interested to act upon, we can see that in the nested sequence there are counts of questions, and the number of correct questions. We will take these two fields (and combine them to make a percent column that represents the percentage of correct answers for the test) and the name of the exam for analysis. The below code will use the function lambda_scores (for the construction of this function see SparkCode.ipynb) to unroll the Json data in this manner and save the result as a temporary table \"scores\".\n",
    "```\n",
    "my_scores = assessments.rdd.flatMap(lambda_scores).toDF()\n",
    "\n",
    "my_scores.registerTempTable('scores')\n",
    "```\n",
    "Finally, we will also create a questions table. Within the nesting in sequences there is a nested list called questions. Within the data for each question is a unique question identifier. Using this question id, along with a sequence id (pertains to the sequence of questions), user id, exam name, and question number we can create a new table questions. We will use the function lambda_questions to do so (built in SparkCode.ipynb) and will save the result as a temporary table \"questions\".\n",
    "```\n",
    "my_questions = assessments.rdd.flatMap(lambda_questions).toDF()\n",
    "\n",
    "my_questions.registerTempTable('questions')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries and Answering Business Questions with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to HDFS with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tearing Down the Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
